{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}\n",
    "reviews = []\n",
    "ratings = []\n",
    "\n",
    "for i in range(0,600,20):\n",
    "    url2 = 'https://www.yelp.com/biz/kims-sushi-west-orange?osq=sushi' + str(i)\n",
    "    response = requests.get(url2, headers=headers, verify=False).text\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    time.sleep(5)\n",
    "    rblock= soup.find_all('div', attrs={'lemon--div__373c0__1mboc arrange-unit__373c0__1piwO arrange-unit-grid-column--8__373c0__2yTAx border-color--default__373c0__2oFDT'})[1:421]\n",
    "    \n",
    "    for block in rblock:\n",
    "        reviews.append(block.find('span',attrs={'lang':'en'}))\n",
    "        ratings.append(block.find('div',class_=re.compile(r\"^i-stars--regular\")))\n",
    "        \n",
    "print(len(reviews))\n",
    "print(len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate=[]\n",
    "\n",
    "cleanr = re.compile('<[^<]+?')\n",
    "review = [re.sub(cleanr, '',a.text) for a in reviews]\n",
    "for r in ratings:\n",
    "    rate.append(r['aria-label'])\n",
    "\n",
    "rating= [i.split(' star rating')[0] for i in rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sushi= pd.DataFrame({'reviews':review, 'ratings':rating})[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sushi.to_csv('sushi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "import collections\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for row in sushi['reviews']:\n",
    "    \n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    words = tokenizer.tokenize(str(row))\n",
    "    \n",
    "    #removing punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    \n",
    "    #removing stop words\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    characters_to_remove = ['...', '', '’', \"n't\", \"'s'\",\n",
    "                            '”', \"\", ' ', '  ','..','.',str(\"'s\"),\".i\",\n",
    "                            '“','``']   \n",
    "   \n",
    "    #tokenizing words\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Lematizing words\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    reviews.append(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = []\n",
    "\n",
    "for lr in reviews:\n",
    "    tokenized_reviews.append(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(tokenized_reviews, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alot', 0.9992790222167969),\n",
       " ('state', 0.9992648363113403),\n",
       " ('thing', 0.999247670173645),\n",
       " ('year', 0.999241292476654),\n",
       " ('restaurant', 0.9992409944534302)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['sushi'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eel', 0.9990442395210266),\n",
       " ('shrimp', 0.9989959597587585),\n",
       " ('tempura', 0.9989181756973267),\n",
       " ('baked', 0.9988822937011719),\n",
       " ('tried', 0.998764157295227)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['crab'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crab', 0.9989959597587585),\n",
       " ('eel', 0.998913049697876),\n",
       " ('ball', 0.9986932873725891),\n",
       " ('roll', 0.9984256029129028),\n",
       " ('cucumber', 0.9983853697776794)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['shrimp'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('hot_topic_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the word2vec model, we can infer that the words associated with the words chosen make sense as the sushi at SushiKim is \"state\"-renowned and you can get \"a lot\" of sushi, and so on. All the words that the model outlies are specific enough to the words searched through the database of tokenized words. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
