{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "t3569s1CX3lS",
    "outputId": "dee8eb50-b1fc-481b-8fa5-5c3fe45e166f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\theot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\theot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\theot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import movie_reviews\n",
    "import collections\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y21X28slUE4m"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-3-aec8d2652953>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-aec8d2652953>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    return dict([(ngram, True) for ngram in s])\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def txt_preprocess(s):\n",
    "    r = []\n",
    "    words = word_tokenize(str(s))\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = ' '.join(word for word in clean_words if not word.isdigit())\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words.split()]\n",
    "    return lemma_list\n",
    "\n",
    "\n",
    "    def bag_of_ngrams(s):\n",
    "    return dict([(ngram, True) for ngram in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hndhe_ChTXk-"
   },
   "outputs": [],
   "source": [
    "nike_rev = pd.read_csv('nike_reviews_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "IvyWu9TtTXlE",
    "outputId": "7e7501e8-c2c3-470b-d9a2-18a381cf3813"
   },
   "outputs": [],
   "source": [
    "nike_rev['sentiment'] = nike_df['ratings'].apply(lambda x: 0 if int(x) <= 3 else 1)\n",
    "\n",
    "nike_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dv2YcPhqcCuT"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(nike_rev['reviews'].values, nike_rev['sentiment'].values, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09Td1luCcOjf"
   },
   "outputs": [],
   "source": [
    "nb_train_feats = []\n",
    "nb_test_feats = []\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    nb_train_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_train[i])),Y_train[i]]))\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    nb_test_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_test[i])),Y_test[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FunrbxV-cC3S"
   },
   "outputs": [],
   "source": [
    "for n in range(0,400):\n",
    "    random.shuffle(nb_test_feats)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(nb_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0-silutcDGn"
   },
   "outputs": [],
   "source": [
    "refsets= collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(nb_test_feats):\n",
    "    refsets[label].add(i)\n",
    "    bayes = classifier.classify(feats)\n",
    "    testsets[bayes].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "0P8a5zK-cDRj",
    "outputId": "60d6775a-b405-4a8a-8e5e-42de8e2dd9c6"
   },
   "outputs": [],
   "source": [
    "print(\"Nike Bayes Metrics\")\n",
    "print('bayes Accuracy', nltk.classify.accuracy(classifier, nb_test_feats))\n",
    "print('neg recall:', recall(refsets[1], testsets[1]))\n",
    "print('neg precision:', precision(refsets[1], testsets[1]))\n",
    "print('neg F-measure:', f_measure(refsets[1], testsets[1]))\n",
    "print('pos recall:', recall(refsets[0], testsets[0]))\n",
    "print('pos precision:', precision(refsets[0], testsets[0]))\n",
    "print('pos F-measure:', f_measure(refsets[0], testsets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEmbPVKkTXlL"
   },
   "outputs": [],
   "source": [
    "facemask_rev = pd.read_csv('facemask_reviews_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "DaKjoogoTXlT",
    "outputId": "a14d0d77-0083-4d73-fe36-95ce63512079"
   },
   "outputs": [],
   "source": [
    "facemask_rev['sentiment'] = facemask_df['rating'].apply(lambda x: 0 if int(x) <= 3 else 1)\n",
    "\n",
    "facemask_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dg_g9B_Re7d8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(facemask_rev['reviews'].values, facemask_rev['sentiment'].values, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiFZT0rke7eQ"
   },
   "outputs": [],
   "source": [
    "nb_train_feats = []\n",
    "nb_test_feats = []\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    nb_train_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_train[i])),Y_train[i]]))\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    nb_test_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_test[i])),Y_test[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXtn55i4e7eY"
   },
   "outputs": [],
   "source": [
    "for n in range(0,320):\n",
    "    random.shuffle(nb_test_feats)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(nb_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBjfxbrWe7ec"
   },
   "outputs": [],
   "source": [
    "refsets= collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(nb_test_feats):\n",
    "    refsets[label].add(i)\n",
    "    bayes = classifier.classify(feats)\n",
    "    testsets[bayes].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "Dxuy_MIXe7ef",
    "outputId": "681a37d4-e55c-49c5-943c-277c1bea64a4"
   },
   "outputs": [],
   "source": [
    "print(\"Facemask Bayes Metrics\")\n",
    "print('bayes Accuracy', nltk.classify.accuracy(classifier, nb_test_feats))\n",
    "print('neg recall:', recall(refsets[1], testsets[1]))\n",
    "print('neg precision:', precision(refsets[1], testsets[1]))\n",
    "print('neg F-measure:', f_measure(refsets[1], testsets[1]))\n",
    "print('pos recall:', recall(refsets[0], testsets[0]))\n",
    "print('pos precision:', precision(refsets[0], testsets[0]))\n",
    "print('pos F-measure:', f_measure(refsets[0], testsets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "10q2BEzXTXll",
    "outputId": "968558e2-5246-4111-bd67-d1efe99a3e71"
   },
   "outputs": [],
   "source": [
    "bh_rev = pd.read_csv('blackhead_reviews_updated.csv')\n",
    "\n",
    "bh_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHb7KBYyZOfv"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(bh_rev['reviews'].values, bh_rev['sentiment'].values, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGAmNrDxZ8Tg"
   },
   "outputs": [],
   "source": [
    "nb_train_feats = []\n",
    "nb_test_feats = []\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    nb_train_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_train[i])),Y_train[i]]))\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    nb_test_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_test[i])),Y_test[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WDO4oK2alMR"
   },
   "outputs": [],
   "source": [
    "for n in range(0,400):\n",
    "    random.shuffle(nb_test_feats)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(nb_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0s7flwvawEG"
   },
   "outputs": [],
   "source": [
    "refsets= collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(nb_test_feats):\n",
    "    refsets[label].add(i)\n",
    "    bayes = classifier.classify(feats)\n",
    "    testsets[bayes].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "fACi4gZKaxbb",
    "outputId": "4061665e-da20-4899-b959-56f436546485"
   },
   "outputs": [],
   "source": [
    "print(\"Blackhead Bayes Metrics\")\n",
    "print('bayes Accuracy', nltk.classify.accuracy(classifier, nb_test_feats))\n",
    "print('neg recall:', recall(refsets[1], testsets[1]))\n",
    "print('neg precision:', precision(refsets[1], testsets[1]))\n",
    "print('neg F-measure:', f_measure(refsets[1], testsets[1]))\n",
    "print('pos recall:', recall(refsets[0], testsets[0]))\n",
    "print('pos precision:', precision(refsets[0], testsets[0]))\n",
    "print('pos F-measure:', f_measure(refsets[0], testsets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "SmwzpCLwTXlp",
    "outputId": "9442fa2e-fe01-474b-f372-1549a37b0a53"
   },
   "outputs": [],
   "source": [
    "apl_rev = pd.read_csv('apple_reviews_updated.csv')\n",
    "\n",
    "apl_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUth-cJIUBBn"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(apl_rev['reviews'].values, apl_rev['sentiment'].values, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wMzlTSfUKza"
   },
   "outputs": [],
   "source": [
    "nb_train_feats = []\n",
    "nb_test_feats = []\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    nb_train_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_train[i])),Y_train[i]]))\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    nb_test_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_test[i])),Y_test[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3w9wHKNUR8S"
   },
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "from nltk.classify import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdpQw1X5UWmN"
   },
   "outputs": [],
   "source": [
    "for n in range(0,400):\n",
    "    random.shuffle(nb_test_feats)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(nb_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ltu8MALUjgL"
   },
   "outputs": [],
   "source": [
    "refsets= collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(nb_test_feats):\n",
    "    refsets[label].add(i)\n",
    "    bayes = classifier.classify(feats)\n",
    "    testsets[bayes].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "JHXFgRk7UoCT",
    "outputId": "bde13b11-040f-4e02-f8be-07bad6914c2b"
   },
   "outputs": [],
   "source": [
    "print(\"Apple Bayes Metrics\")\n",
    "print('bayes Accuracy', nltk.classify.accuracy(classifier, nb_test_feats))\n",
    "print('neg recall:', recall(refsets[1], testsets[1]))\n",
    "print('neg precision:', precision(refsets[1], testsets[1]))\n",
    "print('neg F-measure:', f_measure(refsets[1], testsets[1]))\n",
    "print('pos recall:', recall(refsets[0], testsets[0]))\n",
    "print('pos precision:', precision(refsets[0], testsets[0]))\n",
    "print('pos F-measure:', f_measure(refsets[0], testsets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDG31E_eTXlZ",
    "outputId": "7f627026-7e2d-4be3-ae8f-f435a558c16d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cos_rev = pd.read_csv('cosmetic_reviews.csv')\n",
    "\n",
    "cos_rev['sentiment'] = cos_rev['ratings'].apply(lambda x: 0 if int(x) <= 3 else 1)\n",
    "\n",
    "cos_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKU892lKTXli"
   },
   "outputs": [],
   "source": [
    "cos_rev.to_csv('nike_reviews_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yq3MPFAWiFNX"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(cos_rev['reviews'].values, cos_rev['sentiment'].values, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFt4EJY9iFNn"
   },
   "outputs": [],
   "source": [
    "nb_train_feats = []\n",
    "nb_test_feats = []\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    nb_train_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_train[i])),Y_train[i]]))\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    nb_test_feats.append(tuple([bag_of_ngrams(txt_preprocess(X_test[i])),Y_test[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmaaCnAJiFNv"
   },
   "outputs": [],
   "source": [
    "for n in range(0,400):\n",
    "    random.shuffle(nb_test_feats)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(nb_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7htMxqd3iFN5"
   },
   "outputs": [],
   "source": [
    "refsets= collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(nb_test_feats):\n",
    "    refsets[label].add(i)\n",
    "    bayes = classifier.classify(feats)\n",
    "    testsets[bayes].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "6WxM6n79iFN_",
    "outputId": "d1aa2ff3-01fc-4d85-8508-b2c3e332a0f7"
   },
   "outputs": [],
   "source": [
    "print(\"Bayes Metrics\")\n",
    "print('bayes Accuracy', nltk.classify.accuracy(classifier, nb_test_feats))\n",
    "print('neg recall:', recall(refsets[1], testsets[1]))\n",
    "print('neg precision:', precision(refsets[1], testsets[1]))\n",
    "print('neg F-measure:', f_measure(refsets[1], testsets[1]))\n",
    "print('pos recall:', recall(refsets[0], testsets[0]))\n",
    "print('pos precision:', precision(refsets[0], testsets[0]))\n",
    "print('pos F-measure:', f_measure(refsets[0], testsets[0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Info 368 Group 5 Final Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
